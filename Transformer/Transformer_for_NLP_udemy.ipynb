{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.1.0\n",
      "TensorFlow Dataset Version: 3.1.0\n",
      "Query free memories from all GPUs: nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits\n",
      "Free memory list (MB): [9367, 9183, 5388, 10994]\n",
      "Query names of processes running on the GPU index 0: nvidia-smi --query-compute-apps=process_name --format=csv,noheader,nounits --id=0\n",
      "Names of processes running on the GPU index 0: ['python', '/home/RSabathier/.conda/envs/resml/bin/python']\n",
      "Query names of processes running on the GPU index 1: nvidia-smi --query-compute-apps=process_name --format=csv,noheader,nounits --id=1\n",
      "Names of processes running on the GPU index 1: ['/home/HDriss/miniconda3/envs/geoenv/bin/python', '/home/HDriss/miniconda3/envs/geoenv/bin/python']\n",
      "Query names of processes running on the GPU index 2: nvidia-smi --query-compute-apps=process_name --format=csv,noheader,nounits --id=2\n",
      "Names of processes running on the GPU index 2: ['/home/HDriss/miniconda3/envs/geoenv/bin/python', '/home/HDriss/miniconda3/envs/geoenv/bin/python', '/home/HDriss/miniconda3/envs/geoenv/bin/python']\n",
      "Query names of processes running on the GPU index 3: nvidia-smi --query-compute-apps=process_name --format=csv,noheader,nounits --id=3\n",
      "Names of processes running on the GPU index 3: []\n",
      "Left next 1 GPU(s) unmasked: [3] (from [3] available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"TensorFlow Dataset Version:\", tfds.__version__)\n",
    "\n",
    "from utility import mask_busy_gpus\n",
    "mask_busy_gpus(1)  # randomly select 1 unused GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCD9jwXsLwS_"
   },
   "source": [
    "We import files from our personal google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8Or0sLV5b8t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of europarl_en: 301210536\n",
      "Resumption of the session\n",
      "I declare resumed the session of the European Parliament adjourned on Frid\n",
      "length of europarl_fr: 335706962\n",
      "Reprise de la session\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue \n",
      "length of non_breaking_prefix_en: 110\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "messrs\n",
      "mlle\n",
      "mme\n",
      "mr\n",
      "mrs\n",
      "ms\n",
      "ph\n",
      "prof\n",
      "sr\n",
      "st\n",
      "a.m\n",
      "p.m\n",
      "vs\n",
      "i.e\n",
      "e.g\n",
      "length of non_breaking_prefix_fr: 119\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "mme\n",
      "mlle\n",
      "c.-à-d\n",
      "cf\n",
      "chap\n",
      "e.g\n",
      "al\n",
      "etc\n",
      "ex\n",
      "fig\n",
      "suiv\n",
      "sup\n",
      "suppl\n",
      "tél\n",
      "vol\n",
      "vs\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/europarl-v7.fr-en.en\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "print('length of europarl_en:', len(europarl_en))\n",
    "print(europarl_en[:100])\n",
    "\n",
    "with open(\"./data/europarl-v7.fr-en.fr\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    europarl_fr = f.read()\n",
    "print('length of europarl_fr:', len(europarl_fr))\n",
    "print(europarl_fr[:100])\n",
    "\n",
    "with open(\"./data/nonbreaking_prefix.en\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    non_breaking_prefix_en = f.read()\n",
    "print('length of non_breaking_prefix_en:', len(non_breaking_prefix_en))\n",
    "print(non_breaking_prefix_en)\n",
    "    \n",
    "with open(\"./data/nonbreaking_prefix.fr\",\n",
    "          mode='r',\n",
    "          encoding='utf-8') as f:\n",
    "    non_breaking_prefix_fr = f.read()\n",
    "print('length of non_breaking_prefix_fr:', len(non_breaking_prefix_fr))\n",
    "print(non_breaking_prefix_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwIBeGXn7LIJ"
   },
   "source": [
    "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_TeuktU40Cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_breaking_prefix_en:\n",
      " [' a.', ' b.', ' c.', ' d.', ' e.', ' f.', ' g.', ' h.', ' i.', ' j.', ' k.', ' l.', ' m.', ' n.', ' o.', ' p.', ' q.', ' r.', ' s.', ' t.', ' u.', ' v.', ' w.', ' x.', ' y.', ' z.', ' messrs.', ' mlle.', ' mme.', ' mr.', ' mrs.', ' ms.', ' ph.', ' prof.', ' sr.', ' st.', ' a.m.', ' p.m.', ' vs.', ' i.e.', ' e.g.']\n",
      "non_breaking_prefix_fr:\n",
      " [' a.', ' b.', ' c.', ' d.', ' e.', ' f.', ' g.', ' h.', ' i.', ' j.', ' k.', ' l.', ' m.', ' n.', ' o.', ' p.', ' q.', ' r.', ' s.', ' t.', ' u.', ' v.', ' w.', ' x.', ' y.', ' z.', ' mme.', ' mlle.', ' c.-à-d.', ' cf.', ' chap.', ' e.g.', ' al.', ' etc.', ' ex.', ' fig.', ' suiv.', ' sup.', ' suppl.', ' tél.', ' vol.', ' vs.']\n"
     ]
    }
   ],
   "source": [
    "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
    "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
    "print('non_breaking_prefix_en:\\n', non_breaking_prefix_en)\n",
    "\n",
    "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
    "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]\n",
    "print('non_breaking_prefix_fr:\\n', non_breaking_prefix_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H9x4mZfKMaxD"
   },
   "source": [
    "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qg-8LLK-WdFp"
   },
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "# Add $$$ after non ending sentence points\n",
    "for prefix in non_breaking_prefix_en:\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "# Remove $$$ markers\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "# Clear multiple spaces\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_fr = europarl_fr\n",
    "for prefix in non_breaking_prefix_fr:\n",
    "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
    "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
    "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
    "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
    "corpus_fr = corpus_fr.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p5YXanmOd_xK"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en,\n",
    "                                                                       target_vocab_size=2**13)\n",
    "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_fr,\n",
    "                                                                       target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2  # = 8190\n",
    "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2  # = 8171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
    "           for sentence in corpus_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Remove too long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Inputs/outputs creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvDfLDWUONlE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (790515, 30)\n",
      "input sentence (cleaned, tokenized, padded) #0:\n",
      " [8188 4399  962 2124    3    1 2528 8189    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #1:\n",
      " [8188  555   18 3504    7  178   12   16  439    6    1  452    3    1\n",
      "  274  322 3301    2  357   16  668 7977 2528 7978 8189    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #2:\n",
      " [8188 7303 4717    2 1123    2   10   16 4784   89   13 7798 8033 7978\n",
      " 8189    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #3:\n",
      " [8188 7972   25  334 4789   73    5 5454 7964    7 4784   89   13 7798\n",
      " 8033 7973 8189    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #4:\n",
      " [8188  232   43    2   12    7  173    3 1589 7978 8189    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #5:\n",
      " [8188  228    1  334 2164 8047    2   11  345   59   21   29 6963 3904\n",
      "   28 1117 5159 4432 8032 7978 8189    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #6:\n",
      " [8188  232   43    2   12    7  173    3 1589 7978 8189    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #7:\n",
      " [8188   11   33   57  190 6212   68 1653 1623 1141  548 1451 8032 2733\n",
      " 6565 7978 8189    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #8:\n",
      " [8188  630  205 3053    4  333    8   27  287   90   12 3465 7964    5\n",
      "   24   11   27  250 1651  786 7978 8189    0    0    0    0    0    0\n",
      "    0    0]\n",
      "input sentence (cleaned, tokenized, padded) #9:\n",
      " [8188   47 2201    8   16   35   14  373 1084    1  445    3 6860 7964\n",
      " 3452 7978 8189    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "outputs shape: (790515, 30)\n",
      "output sentence (cleaned, tokenized, padded) #0:\n",
      " [8169 1248  800    1    3 5731 8170    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #1:\n",
      " [8169  609  324 4173 7945   16  187    6   21  351   17    7 6278 2321\n",
      "    2   23  249    1   35  559    1 5731 7959 8170    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #2:\n",
      " [8169   34   55 1682    6   55 4965 7945   19   35 5403 7945    1 7187\n",
      " 7959 8170    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #3:\n",
      " [8169 7953   54  250    2  430 2133 8029    2 1710   31   14 5403 7945\n",
      "    1 7187 7954 8170    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #4:\n",
      " [8169  157    3  304    2 8012 7952   12   14 4128    1 2005 7959 8170\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #5:\n",
      " [8169  234 8021 7952  435   10   12 8013 7952  682    2   30 2727 7945\n",
      "   59 7990   43 7982 1013  592 8021 7952   22 3952  144 7959 8170    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #6:\n",
      " [8169  157    3  304    2 8012 7952   12   14 4128    1 2005 7959 8170\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #7:\n",
      " [8169   34  108   55  553   16 4418   23  351    1 8021 7952  386 1789\n",
      " 7964    2   20  163 8021 7952 6297  415 1013 2379 7959 8170    0    0\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #8:\n",
      " [8169 3350  103 1239   24   16  351   20   12    6 8021 7952  366   13\n",
      "  318   13 7360    4    9   30 6001 1316  115   14  272 1026 7959 8170\n",
      "    0    0]\n",
      "output sentence (cleaned, tokenized, padded) #9:\n",
      " [8169   42 4060    9  125  226   37   60 1310    5  360    1  727 4115\n",
      " 3469 7959 8170    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)\n",
    "\n",
    "print('inputs shape:', inputs.shape)\n",
    "for i, in_sent in enumerate(inputs[:10]):\n",
    "    print('input sentence (cleaned, tokenized, padded) #%d:\\n %s' % (i, in_sent))\n",
    "    \n",
    "print('outputs shape:', outputs.shape)\n",
    "for i, out_sent in enumerate(outputs[:10]):\n",
    "    print('output sentence (cleaned, tokenized, padded) #%d:\\n %s' % (i, out_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycT0YqydRcUd"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SBoH8G4XyR9"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7G9C3ucmJ86I"
   },
   "source": [
    "Positional encoding formulae:\n",
    "\n",
    "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
    "\n",
    "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2wc6sYlX0dr"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcw8YIQqRhOJ"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sffhwwvX-wj"
   },
   "source": [
    "### Attention computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VBuW6lESLDX"
   },
   "source": [
    "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rEoCNJURbrT"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MjtvXrfYEx7"
   },
   "source": [
    "### Multi-head attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvq4I9uTX5p7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size):  # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape)  # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])  # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yiyuHe1OeT5N"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UV0ZMH7KT_KZ"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-P92KeZih60"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DthraBEwuvl"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZWZyFBnwy8u"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combined with encoder output\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpzdiWHiwywF"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5sJYkjbz5DD"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqvqNjJPwyh-"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c-LRThUPrso"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128  # 512\n",
    "NB_LAYERS = 4  # 6\n",
    "FFN_UNITS = 512  # 2048\n",
    "NB_PROJ = 8  # 8\n",
    "DROPOUT_RATE = 0.1  # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46xg4Wrg1Wgl"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Goque362343"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Nb_32PIU5Zkh",
    "outputId": "ab083fb5-19d1-49e0-ab50-bb41bc85f1e0"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhFK5kUx602K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 5.7451 Accuracy 0.0000\n",
      "Epoch 1 Batch 1000 Loss 4.2600 Accuracy 0.0946\n",
      "Epoch 1 Batch 2000 Loss 3.5386 Accuracy 0.1447\n",
      "Epoch 1 Batch 3000 Loss 3.1652 Accuracy 0.1749\n",
      "Epoch 1 Batch 4000 Loss 2.8850 Accuracy 0.2068\n",
      "Epoch 1 Batch 5000 Loss 2.6547 Accuracy 0.2348\n",
      "Epoch 1 Batch 6000 Loss 2.4553 Accuracy 0.2578\n",
      "Epoch 1 Batch 7000 Loss 2.2967 Accuracy 0.2776\n",
      "Epoch 1 Batch 8000 Loss 2.1635 Accuracy 0.2944\n",
      "Epoch 1 Batch 9000 Loss 2.0533 Accuracy 0.3097\n",
      "Epoch 1 Batch 10000 Loss 1.9812 Accuracy 0.3187\n",
      "Epoch 1 Batch 11000 Loss 1.9259 Accuracy 0.3248\n",
      "Epoch 1 Batch 12000 Loss 1.8786 Accuracy 0.3296\n",
      "Saving checkpoint for epoch 1 at ./ckpt/ckpt-1\n",
      "Time taken for 1 epoch: 5059.8174 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 1.2389 Accuracy 0.3847\n",
      "Epoch 2 Batch 1000 Loss 1.2968 Accuracy 0.3898\n",
      "Epoch 2 Batch 2000 Loss 1.2756 Accuracy 0.3960\n",
      "Epoch 2 Batch 3000 Loss 1.2438 Accuracy 0.4025\n",
      "Epoch 2 Batch 4000 Loss 1.2027 Accuracy 0.4144\n",
      "Epoch 2 Batch 5000 Loss 1.1722 Accuracy 0.4223\n",
      "Epoch 2 Batch 6000 Loss 1.1396 Accuracy 0.4268\n",
      "Epoch 2 Batch 7000 Loss 1.1172 Accuracy 0.4308\n",
      "Epoch 2 Batch 8000 Loss 1.0951 Accuracy 0.4346\n",
      "Epoch 2 Batch 9000 Loss 1.0765 Accuracy 0.4387\n",
      "Epoch 2 Batch 10000 Loss 1.0805 Accuracy 0.4383\n",
      "Epoch 2 Batch 11000 Loss 1.0902 Accuracy 0.4362\n",
      "Epoch 2 Batch 12000 Loss 1.0986 Accuracy 0.4338\n",
      "Saving checkpoint for epoch 2 at ./ckpt/ckpt-2\n",
      "Time taken for 1 epoch: 8933.4171 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.0880 Accuracy 0.3669\n",
      "Epoch 3 Batch 1000 Loss 1.1471 Accuracy 0.4139\n",
      "Epoch 3 Batch 2000 Loss 1.1342 Accuracy 0.4183\n",
      "Epoch 3 Batch 3000 Loss 1.1097 Accuracy 0.4247\n",
      "Epoch 3 Batch 4000 Loss 1.0731 Accuracy 0.4359\n",
      "Epoch 3 Batch 5000 Loss 1.0480 Accuracy 0.4431\n",
      "Epoch 3 Batch 6000 Loss 1.0210 Accuracy 0.4466\n",
      "Epoch 3 Batch 7000 Loss 1.0030 Accuracy 0.4499\n",
      "Epoch 3 Batch 8000 Loss 0.9854 Accuracy 0.4528\n",
      "Epoch 3 Batch 9000 Loss 0.9705 Accuracy 0.4563\n",
      "Epoch 3 Batch 10000 Loss 0.9781 Accuracy 0.4555\n",
      "Epoch 3 Batch 11000 Loss 0.9910 Accuracy 0.4527\n",
      "Epoch 3 Batch 12000 Loss 1.0026 Accuracy 0.4498\n",
      "Saving checkpoint for epoch 3 at ./ckpt/ckpt-3\n",
      "Time taken for 1 epoch: 8958.8137 secs\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.1241 Accuracy 0.4423\n",
      "Epoch 4 Batch 1000 Loss 1.0891 Accuracy 0.4233\n",
      "Epoch 4 Batch 2000 Loss 1.0770 Accuracy 0.4278\n",
      "Epoch 4 Batch 3000 Loss 1.0521 Accuracy 0.4341\n",
      "Epoch 4 Batch 4000 Loss 1.0171 Accuracy 0.4451\n",
      "Epoch 4 Batch 5000 Loss 0.9929 Accuracy 0.4522\n",
      "Epoch 4 Batch 6000 Loss 0.9676 Accuracy 0.4556\n",
      "Epoch 4 Batch 7000 Loss 0.9511 Accuracy 0.4587\n",
      "Epoch 4 Batch 8000 Loss 0.9349 Accuracy 0.4614\n",
      "Epoch 4 Batch 9000 Loss 0.9213 Accuracy 0.4647\n",
      "Epoch 4 Batch 10000 Loss 0.9296 Accuracy 0.4636\n",
      "Epoch 4 Batch 11000 Loss 0.9435 Accuracy 0.4607\n",
      "Epoch 4 Batch 12000 Loss 0.9563 Accuracy 0.4576\n",
      "Saving checkpoint for epoch 4 at ./ckpt/ckpt-4\n",
      "Time taken for 1 epoch: 8911.9390 secs\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.0792 Accuracy 0.4235\n",
      "Epoch 5 Batch 1000 Loss 1.0552 Accuracy 0.4294\n",
      "Epoch 5 Batch 2000 Loss 1.0431 Accuracy 0.4338\n",
      "Epoch 5 Batch 3000 Loss 1.0183 Accuracy 0.4395\n",
      "Epoch 5 Batch 4000 Loss 0.9844 Accuracy 0.4508\n",
      "Epoch 5 Batch 5000 Loss 0.9606 Accuracy 0.4577\n",
      "Epoch 5 Batch 6000 Loss 0.9360 Accuracy 0.4611\n",
      "Epoch 5 Batch 7000 Loss 0.9196 Accuracy 0.4640\n",
      "Epoch 5 Batch 8000 Loss 0.9042 Accuracy 0.4666\n",
      "Epoch 5 Batch 9000 Loss 0.8915 Accuracy 0.4698\n",
      "Epoch 5 Batch 10000 Loss 0.9000 Accuracy 0.4686\n",
      "Epoch 5 Batch 11000 Loss 0.9145 Accuracy 0.4656\n",
      "Epoch 5 Batch 12000 Loss 0.9274 Accuracy 0.4625\n",
      "Saving checkpoint for epoch 5 at ./ckpt/ckpt-5\n",
      "Time taken for 1 epoch: 7267.0086 secs\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.0340 Accuracy 0.4650\n",
      "Epoch 6 Batch 1000 Loss 1.0318 Accuracy 0.4327\n",
      "Epoch 6 Batch 2000 Loss 1.0205 Accuracy 0.4373\n",
      "Epoch 6 Batch 3000 Loss 0.9950 Accuracy 0.4434\n",
      "Epoch 6 Batch 4000 Loss 0.9612 Accuracy 0.4545\n",
      "Epoch 6 Batch 5000 Loss 0.9382 Accuracy 0.4617\n",
      "Epoch 6 Batch 6000 Loss 0.9140 Accuracy 0.4649\n",
      "Epoch 6 Batch 7000 Loss 0.8980 Accuracy 0.4678\n",
      "Epoch 6 Batch 8000 Loss 0.8828 Accuracy 0.4704\n",
      "Epoch 6 Batch 9000 Loss 0.8702 Accuracy 0.4735\n",
      "Epoch 6 Batch 10000 Loss 0.8793 Accuracy 0.4722\n",
      "Epoch 6 Batch 11000 Loss 0.8935 Accuracy 0.4691\n",
      "Epoch 6 Batch 12000 Loss 0.9071 Accuracy 0.4659\n",
      "Saving checkpoint for epoch 6 at ./ckpt/ckpt-6\n",
      "Time taken for 1 epoch: 3279.4432 secs\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.0320 Accuracy 0.4407\n",
      "Epoch 7 Batch 1000 Loss 1.0125 Accuracy 0.4360\n",
      "Epoch 7 Batch 2000 Loss 1.0026 Accuracy 0.4402\n",
      "Epoch 7 Batch 3000 Loss 0.9787 Accuracy 0.4466\n",
      "Epoch 7 Batch 4000 Loss 0.9445 Accuracy 0.4575\n",
      "Epoch 7 Batch 5000 Loss 0.9208 Accuracy 0.4644\n",
      "Epoch 7 Batch 6000 Loss 0.8971 Accuracy 0.4676\n",
      "Epoch 7 Batch 7000 Loss 0.8816 Accuracy 0.4705\n",
      "Epoch 7 Batch 8000 Loss 0.8667 Accuracy 0.4730\n",
      "Epoch 7 Batch 9000 Loss 0.8542 Accuracy 0.4762\n",
      "Epoch 7 Batch 10000 Loss 0.8632 Accuracy 0.4748\n",
      "Epoch 7 Batch 11000 Loss 0.8780 Accuracy 0.4718\n",
      "Epoch 7 Batch 12000 Loss 0.8915 Accuracy 0.4685\n",
      "Saving checkpoint for epoch 7 at ./ckpt/ckpt-7\n",
      "Time taken for 1 epoch: 3286.9958 secs\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Batch 0 Loss 0.9208 Accuracy 0.4300\n",
      "Epoch 8 Batch 1000 Loss 1.0008 Accuracy 0.4381\n",
      "Epoch 8 Batch 2000 Loss 0.9896 Accuracy 0.4426\n",
      "Epoch 8 Batch 3000 Loss 0.9652 Accuracy 0.4486\n",
      "Epoch 8 Batch 4000 Loss 0.9309 Accuracy 0.4597\n",
      "Epoch 8 Batch 5000 Loss 0.9080 Accuracy 0.4667\n",
      "Epoch 8 Batch 6000 Loss 0.8844 Accuracy 0.4699\n",
      "Epoch 8 Batch 7000 Loss 0.8691 Accuracy 0.4728\n",
      "Epoch 8 Batch 8000 Loss 0.8545 Accuracy 0.4752\n",
      "Epoch 8 Batch 9000 Loss 0.8421 Accuracy 0.4783\n",
      "Epoch 8 Batch 10000 Loss 0.8512 Accuracy 0.4770\n",
      "Epoch 8 Batch 11000 Loss 0.8661 Accuracy 0.4740\n",
      "Epoch 8 Batch 12000 Loss 0.8798 Accuracy 0.4706\n",
      "Saving checkpoint for epoch 8 at ./ckpt/ckpt-8\n",
      "Time taken for 1 epoch: 3252.9655 secs\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Batch 0 Loss 0.8858 Accuracy 0.3949\n",
      "Epoch 9 Batch 1000 Loss 0.9881 Accuracy 0.4398\n",
      "Epoch 9 Batch 2000 Loss 0.9785 Accuracy 0.4445\n",
      "Epoch 9 Batch 3000 Loss 0.9544 Accuracy 0.4508\n",
      "Epoch 9 Batch 4000 Loss 0.9200 Accuracy 0.4616\n",
      "Epoch 9 Batch 5000 Loss 0.8969 Accuracy 0.4686\n",
      "Epoch 9 Batch 6000 Loss 0.8737 Accuracy 0.4717\n",
      "Epoch 9 Batch 7000 Loss 0.8584 Accuracy 0.4745\n",
      "Epoch 9 Batch 8000 Loss 0.8439 Accuracy 0.4771\n",
      "Epoch 9 Batch 9000 Loss 0.8319 Accuracy 0.4801\n",
      "Epoch 9 Batch 10000 Loss 0.8411 Accuracy 0.4788\n",
      "Epoch 9 Batch 11000 Loss 0.8560 Accuracy 0.4757\n",
      "Epoch 9 Batch 12000 Loss 0.8698 Accuracy 0.4723\n",
      "Saving checkpoint for epoch 9 at ./ckpt/ckpt-9\n",
      "Time taken for 1 epoch: 3245.2903 secs\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 Batch 0 Loss 1.0521 Accuracy 0.4488\n",
      "Epoch 10 Batch 1000 Loss 0.9817 Accuracy 0.4419\n",
      "Epoch 10 Batch 2000 Loss 0.9696 Accuracy 0.4459\n",
      "Epoch 10 Batch 3000 Loss 0.9459 Accuracy 0.4522\n",
      "Epoch 10 Batch 4000 Loss 0.9115 Accuracy 0.4632\n",
      "Epoch 10 Batch 5000 Loss 0.8889 Accuracy 0.4702\n",
      "Epoch 10 Batch 6000 Loss 0.8650 Accuracy 0.4733\n",
      "Epoch 10 Batch 7000 Loss 0.8498 Accuracy 0.4761\n",
      "Epoch 10 Batch 8000 Loss 0.8354 Accuracy 0.4786\n",
      "Epoch 10 Batch 9000 Loss 0.8234 Accuracy 0.4816\n",
      "Epoch 10 Batch 10000 Loss 0.8327 Accuracy 0.4802\n",
      "Epoch 10 Batch 11000 Loss 0.8477 Accuracy 0.4772\n",
      "Epoch 10 Batch 12000 Loss 0.8612 Accuracy 0.4738\n",
      "Saving checkpoint for epoch 10 at ./ckpt/ckpt-10\n",
      "Time taken for 1 epoch: 3235.4262 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {:.4f} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  1841408   \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            multiple                  2104192   \n",
      "_________________________________________________________________\n",
      "lin_ouput (Dense)            multiple                  1054059   \n",
      "=================================================================\n",
      "Total params: 4,999,659\n",
      "Trainable params: 4,999,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNHwJJrz3lPB"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    \n",
    "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)\n",
    "        \n",
    "        prediction = predictions[:, -1:, :]\n",
    "        \n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_FR-1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6VeFKrE6Kdx"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence = tokenizer_fr.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
    "    )\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ZdoWKbCP7Czs",
    "outputId": "91523420-98f6-4c0d-b336-643cf6f43308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: This is a really powerful tool!\n",
      "Predicted translation: C'est un instrument vraiment fort !\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer_for_NLP_udemy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
